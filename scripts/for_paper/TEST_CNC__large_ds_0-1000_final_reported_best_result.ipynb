{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from astropy.units import one\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten , Dropout, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.layers import LSTM, GRU, CuDNNLSTM, CuDNNGRU\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import concatenate as Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.contrib.opt import MomentumWOptimizer, AdamWOptimizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, precision_recall_curve, average_precision_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.engine.topology import Layer\n",
    "\n",
    "from tensorflow.keras.layers import Layer \n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate, InputSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # memory footprint support libraries/code\n",
    "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "# !pip install gputil\n",
    "# !pip install psutil\n",
    "# !pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_on_colab = False\n",
    "\n",
    "is_show_colab_env_info = True\n",
    "\n",
    "\n",
    "\n",
    "if(is_on_colab):\n",
    "\n",
    "    if(is_show_colab_env_info):\n",
    "        import psutil\n",
    "        import humanize\n",
    "        import os\n",
    "        import GPUtil as GPU\n",
    "        GPUs = GPU.getGPUs()\n",
    "        # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "        gpu = GPUs[0]\n",
    "        \n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "        #GPU count and name\n",
    "        !lscpu |grep 'Model name'\n",
    "        !nvidia-smi -L\n",
    "        printm()\n",
    "  \n",
    "  \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    gdrive_home_dir = '/content/drive/My Drive/'\n",
    "\n",
    "    project_dir = f'{gdrive_home_dir}DeepCNC/'\n",
    "    ds_dir = f'{project_dir}data/'\n",
    "    snapshot_dir = f'{project_dir}snapshots/'\n",
    "\n",
    "    !ls '{ds_dir}'\n",
    "\n",
    "else:\n",
    "    project_dir = f'/disks/data/paper_projects/archive/DeepCNC/'\n",
    "    ds_dir = f'{project_dir}data/'\n",
    "    snapshot_dir = f'{project_dir}snapshots/'\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_dir = '../'\n",
    "# ds_dir = f'{project_dir}data/'\n",
    "# snapshot_dir = f'{project_dir}snapshots/'\n",
    "    \n",
    "#fnmInput = f'{ds_dir}inputSeq.txt';\n",
    "fnmInput = f'{ds_dir}inputSeq_N_ReplacedBy_C.txt';\n",
    "\n",
    "fnmLabel= f'{ds_dir}inputLabel.txt'\n",
    "\n",
    "# fnmInput = f'{ds_dir}input.txt'; \n",
    "# fnmLabel= f'{ds_dir}inputLabel_0-1.txt'\n",
    "\n",
    "\n",
    "# input_original.txt has 5 nucleotides, so we should use input.txt\n",
    "# inputLabel.txt's contents (0 and 1) need to be swapped because of Keras\n",
    "\n",
    "\n",
    "# fnmInput='./data/input_Test.txt';\n",
    "# fnmLabel ='./data/inputLabel_Test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37274\n",
      "37274\n"
     ]
    }
   ],
   "source": [
    "with open(fnmInput,'r') as f:\n",
    "    print(len(f.readlines()))\n",
    "    \n",
    "    \n",
    "with open(fnmLabel,'r') as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18787 18487\n"
     ]
    }
   ],
   "source": [
    "########################### Stratified k-fold Cross validation: IMBALANCED SET ###############\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(input_features, input_labels,\n",
    "#                                                                                 test_size=hyper_test_split,\n",
    "#                                                                                 stratify=input_labels,\n",
    "#                                                                                 random_state=42)\n",
    "\n",
    "useFullDataSet = 1;\n",
    "ratio_High_Low = 10;\n",
    "\n",
    "with open(fnmLabel) as f:\n",
    "    labels = [ int(x) for x in f.read().split()]\n",
    "    \n",
    "pos_samplecount = labels.count(0)\n",
    "neg_samplecount = labels.count(1)\n",
    "print(pos_samplecount, neg_samplecount)\n",
    "\n",
    "\n",
    "totPos = pos_samplecount\n",
    "totNeg = neg_samplecount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totPos:  18787  totNeg:  18487\n"
     ]
    }
   ],
   "source": [
    "#totPos = 2796;\n",
    "#totNeg = 90923;  # 93719- 2796;\n",
    "\n",
    "totPosNeg = totPos + totNeg;\n",
    "print(\"totPos: \", totPos, \" totNeg: \", totNeg)\n",
    "stepSize = 1;\n",
    "\n",
    "posIndex = [ i for i in range(len(labels)) if labels[i] == 0 ]\n",
    "negIndex = [ i for i in range(len(labels)) if labels[i] == 1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing index\n",
      "size of pos : 18787\n",
      "size of neg : 18487\n",
      " ration:  0\n",
      "After selection size of pos : 18787\n",
      "After selection size of neg : 18487\n",
      " len of merge:  37274\n"
     ]
    }
   ],
   "source": [
    "#posIndex = [i for i in range(0, totPos, stepSize)];\n",
    "#negIndex = [j for j in range(totPos, totPosNeg, stepSize)];\n",
    "\n",
    "print(\"Printing index\")\n",
    "\n",
    "sizePos = len(posIndex)\n",
    "sizeNeg = len(negIndex)\n",
    "\n",
    "print(\"size of pos :\", sizePos)\n",
    "print(\"size of neg :\", sizeNeg)\n",
    "\n",
    "negIndexShuffled = negIndex\n",
    "random.shuffle(negIndexShuffled)\n",
    "\n",
    "ratio_High_Low_Max = math.floor(sizeNeg/ sizePos)\n",
    "\n",
    "if ratio_High_Low > ratio_High_Low_Max:\n",
    "    ratio_High_Low = ratio_High_Low_Max;\n",
    "\n",
    "print( ' ration: ' , ratio_High_Low)\n",
    "\n",
    "\n",
    "if useFullDataSet:\n",
    "    mergedIndex = posIndex +  negIndexShuffled ;\n",
    "    print(\"After selection size of pos :\", len(posIndex))\n",
    "    print(\"After selection size of neg :\", len(negIndexShuffled ))\n",
    "    print( ' len of merge: ' , len(mergedIndex) )\n",
    "    \n",
    "    \n",
    "else:\n",
    "    posIndex_Selected = posIndex\n",
    "    negIndexShuffled_Selected = negIndexShuffled[0:totPos * ratio_High_Low]\n",
    "    mergedIndex = posIndex_Selected +  negIndexShuffled_Selected ;\n",
    "    print(\"After selection size of pos :\", len(posIndex_Selected))\n",
    "    print(\"After selection size of neg :\", len(negIndexShuffled_Selected))\n",
    "    print( ' len of merged : ' , len(mergedIndex) )\n",
    "\n",
    "# sequences_New = sequences[mergedIndex]\n",
    "# labels_New  = labels[mergedIndex] \n",
    "# inp = input_features[mergedIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Seq found: 37274\n",
      "Total Seq found AFTER selection: 37274\n",
      "Finally all nucleotides: \n",
      "4\n",
      "classes:  ['A', 'C', 'G', 'T']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sequences_Orig = [ (line.strip())[15:-15] for line in open(fnmInput)] # .rstrip('\\n')\n",
    "#sequences_Orig = [ (line.strip()) for line in open(fnmInput)] # .rstrip('\\n')\n",
    "sequences_Orig = [ line.strip()[:1000] for line in open(fnmInput)] # .rstrip('\\n')\n",
    "sequences_Orig = list(filter(None, sequences_Orig))  # This removes empty sequences.\n",
    "\n",
    "print('Total Seq found:' , len(sequences_Orig) )\n",
    "\n",
    "sequences = [ sequences_Orig[i] for i in mergedIndex ]\n",
    "print('Total Seq found AFTER selection:' , len(sequences) )\n",
    "\n",
    "\n",
    "# Let's print the first few sequences.\n",
    "df = pd.DataFrame(sequences_Orig, index=np.arange(1, len(sequences_Orig) + 1), columns=['Sequences'])\n",
    "# print(df.head())\n",
    "setNucleotide= set()\n",
    "for seq in sequences_Orig:\n",
    "    s = set (list(seq))\n",
    "    setNucleotide = setNucleotide | s\n",
    "#   print( len(sequence) )\n",
    "\n",
    "print(\"Finally all nucleotides: \")\n",
    "totalNuclType= setNucleotide.__len__()\n",
    "print(totalNuclType)\n",
    "\n",
    "with open(fnmInput ,'rt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = [c.strip() for c in content]\n",
    "content = \"\".join(content)\n",
    "alphabet = list(set(content))\n",
    "integer_encoder = LabelEncoder()\n",
    "integer_encoder.fit(alphabet)\n",
    "print(\"classes: \" ,list(integer_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding of input DONE\n"
     ]
    }
   ],
   "source": [
    "####################  Encode Input Sequence ##############\n",
    "one_hot_encoder = OneHotEncoder(categories=[range(len(integer_encoder.classes_))])\n",
    "input_features = []\n",
    "myBuffer = ''\n",
    "count =1;\n",
    "for sequence in sequences:\n",
    "    integer_encoded = integer_encoder.transform(list(sequence))\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features.append(one_hot_encoded.toarray())\n",
    "    # myBuffer += (\"Seq no:\" + str(count) + \"  len: \" + str(len(sequence))  + \" array len: \" + str( len(one_hot_encoded.toarray()))  + \"\\n\" )\n",
    "    count = count + 1\n",
    "\n",
    "print( \"Encoding of input DONE\")   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Encoding Shape\n",
      "{(1000, 4)}\n",
      "37274\n",
      "Example sequence\n",
      "-----------------------\n",
      "DNA Sequence #1:\n",
      " GAGTGAATTA ... CTGGAAGGCT\n",
      "One hot encoding of Sequence #1:\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print( \"Check Encoding Shape\")\n",
    "lengths = [f.shape for f in input_features]\n",
    "print(set(lengths))    \n",
    "    \n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.stack( input_features )\n",
    "\n",
    "print( len(input_features ) )\n",
    "\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n' ,sequences[0][:10] ,'...' ,sequences[0][-10:])\n",
    "print('One hot encoding of Sequence #1:\\n' ,input_features[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      " [['0' '0' '0' ... '1' '1' '1']]\n",
      "One-hot encoded labels:\n",
      " [[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 1.]]\n",
      "37274\n"
     ]
    }
   ],
   "source": [
    "####################  Encode Label ##############\n",
    "\n",
    "# def processLabel():\n",
    "# LABELS_URL = 'https://raw.githubusercontent.com/abidlabs/deep-learning-genomics-primer/master/labels.txt'\n",
    "# labels = requests.get(LABELS_URL).text.split('\\n')\n",
    "# labels = list(filter(None, labels))  # removes empty sequences\n",
    "labels_Orig = [line.rstrip('\\n') for line in open(fnmLabel)]\n",
    "labels_Orig = list(filter(None, labels_Orig))  # This removes empty sequences.\n",
    "\n",
    "labels = [ labels_Orig[i] for i in mergedIndex]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "labels = np.array(labels).reshape(-1, 1)\n",
    "input_labels = one_hot_encoder.fit_transform(labels).toarray()\n",
    "\n",
    "print('Labels:\\n', labels.T)\n",
    "print('One-hot encoded labels:\\n', input_labels.T)\n",
    "print( len(input_labels ) )\n",
    "\n",
    "labels = np.array(list(map(int, labels)))  # added for sigmoid activation\n",
    "\n",
    "####################  Encode Label End ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=  10 # 10 # 2\n",
    "hyper_epoch=  500 #50 # 5\n",
    "class_weight = {0: 1., 1: 1.} # 10 \n",
    "\n",
    "#class_weight = {0: ratio_High_Low, 1: 1.} # 10 \n",
    "#print(ratio_High_Low)\n",
    "\n",
    "hyper_classes=2\n",
    "\n",
    "hyper_validation_split = 0.20\n",
    "#hyper_test_split=0.10\n",
    "#hyper_pool_size = (1,2)\n",
    "hyper_batch_size = 256\n",
    "# hyper_optimizer = 'Adadelta'\n",
    "# hyper_init_mode = 'zero'\n",
    "# hyper_activation = 'tanh'\n",
    "# hyper_filter = 50\n",
    "# hyper_filterL = 30\n",
    "# hyper_filterW = 32\n",
    "# hyper_neurons = 32\n",
    "# hyper_dropout_rate = 0.1\n",
    "\n",
    "\n",
    "model_history = []\n",
    "dfHistory = pd.DataFrame( columns=['Precsion' , 'Recall' , 'Specificity' , 'Accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "\n",
    "# https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        #clear_output(wait=True)\n",
    "        plt.cla()\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# # https://keras.io/metrics/\n",
    "# def my_pred(y_true, y_pred):\n",
    "#     #return K.mean(y_pred)\n",
    "\n",
    "# #     cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     #print('Confusion matrix:\\n' ,cm)\n",
    "#     # cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "#     total1=sum(sum(cm))\n",
    "#     accuracy1=(tp+tn)/total1\n",
    "#     sensitivity1 = tp/(tp+fn)\n",
    "#     specificity1 = tn/(fp+tn)\n",
    "#     if (tp)==0:\n",
    "#         precision1 = 0\n",
    "#     else:\n",
    "#         precision1 = tp / (tp + fp)\n",
    "\n",
    "#     MCC = (tp*tn - fp*fn) / np.sqrt(  (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  ) \n",
    "#     return MCC\n",
    "\n",
    "# # model.compile(optimizer='rmsprop',  loss='binary_crossentropy',   metrics=['accuracy', my_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "        #clear_output(wait=True)\n",
    "        plt.cla()\n",
    "        plt.plot(self.history['lr'], label=\"learning rate\")\n",
    "        plt.legend()\n",
    "        plt.show();   \n",
    "        \n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # ------------------------------------------ sequential definition --------------------------------------------------------------------        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def make_seq_cnn():\n",
    "        # regularization: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8 ,input_shape=( inputSeqLength, totalNuclType ))  )\n",
    "        model.add(Conv1D(filters=64, kernel_size=12 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.5)) # input = 129 * 32\n",
    "        model.add(MaxPooling1D(pool_size= 2 ))#totalNuclType)) # input = 129 * 32 , output= 32*32 (filter/pool_size* filterTotal)\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "        model.add(Conv1D(filters=48, kernel_size=12, kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.5)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "        model.add(MaxPooling1D(pool_size=2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32\n",
    "\n",
    "#         #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "#         model.add(Conv1D(filters=32, kernel_size=12, kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "#         model.add(BatchNormalization())    \n",
    "#         model.add(Activation('relu'))\n",
    "#         model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "#         model.add(MaxPooling1D(pool_size = 2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32    \n",
    "\n",
    "    #     model.add(Conv1D(filters=92, kernel_size=3))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "    #     #model.add(Conv1D(filters=32, kernel_size=12 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-5), bias_regularizer=l2(4e-5) )  )    \n",
    "    #     #model.add(BatchNormalization())    \n",
    "    #     model.add(Activation('relu'))\n",
    "    #     model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "    #     model.add(MaxPooling1D(pool_size=totalNuclType))  # input = 25 *32 , output 6 * 32  \n",
    "\n",
    "\n",
    "        #model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "        #model.add(Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(1e-6), recurrent_regularizer=l2(1e-6), bias_regularizer=l2(1e-6)) ))\n",
    "        #model.add(Dropout(rate = 0.4))    \n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        #model.add(Dense(16))\n",
    "        #  cite: use weight decay increasing from 0 to 1E-5 at 1200 epochs, to 1E-4 at 2500 epochs, and to 1E-3 at 400 epochs. \n",
    "        # [â€¦] The surface is smoother and transitions are more gradual\n",
    "        model.add(Dense(32, kernel_regularizer=l2(0.02) ) )\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.6)) \n",
    "        model.add(Dense(2, kernel_regularizer=l2(0.02)))        \n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        SGDRScheduler_config = { 'min_lr':2e-4,\n",
    "                                 'max_lr':.2e-3,\n",
    "                                 'steps_per_epoch':np.ceil(hyper_epoch/(hyper_batch_size)),\n",
    "                                 'lr_decay':0.95,\n",
    "                                 'cycle_length': 5,\n",
    "                                 'mult_factor': 1.5}\n",
    "        \n",
    "        config = {'learning_rate':0.001, 'SGDRScheduler_config': SGDRScheduler_config}\n",
    "        \n",
    "        return [model, config]\n",
    "\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # ------------------------------------------ sequential definition --------------------------------------------------------------------        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def make_seq_3mer_cnn():\n",
    "        # regularization: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv1D(filters=92, kernel_size=3 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.5)) # input = 129 * 32\n",
    "        model.add(MaxPooling1D(pool_size= 2 ))#totalNuclType)) # input = 129 * 32 , output= 32*32 (filter/pool_size* filterTotal)\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "        model.add(Conv1D(filters=48, kernel_size=3, kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.5)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "        model.add(MaxPooling1D(pool_size=2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32\n",
    "\n",
    "#         #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "#         model.add(Conv1D(filters=32, kernel_size=12, kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "#         model.add(BatchNormalization())    \n",
    "#         model.add(Activation('relu'))\n",
    "#         model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "#         model.add(MaxPooling1D(pool_size = 2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32    \n",
    "\n",
    "    #     model.add(Conv1D(filters=92, kernel_size=3))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "    #     #model.add(Conv1D(filters=32, kernel_size=12 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-5), bias_regularizer=l2(4e-5) )  )    \n",
    "    #     #model.add(BatchNormalization())    \n",
    "    #     model.add(Activation('relu'))\n",
    "    #     model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "    #     model.add(MaxPooling1D(pool_size=totalNuclType))  # input = 25 *32 , output 6 * 32  \n",
    "\n",
    "\n",
    "        #model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "        #model.add(Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(1e-6), recurrent_regularizer=l2(1e-6), bias_regularizer=l2(1e-6)) ))\n",
    "        #model.add(Dropout(rate = 0.4))    \n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        #model.add(Dense(16))\n",
    "        #  cite: use weight decay increasing from 0 to 1E-5 at 1200 epochs, to 1E-4 at 2500 epochs, and to 1E-3 at 400 epochs. \n",
    "        # [â€¦] The surface is smoother and transitions are more gradual\n",
    "        model.add(Dense(48, kernel_regularizer=l2(0.02) ) )\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.6)) \n",
    "        model.add(Dense(2, kernel_regularizer=l2(0.02)))        \n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        SGDRScheduler_config = { 'min_lr':2e-4,\n",
    "                                 'max_lr':5e-4,\n",
    "                                 'steps_per_epoch':np.ceil(hyper_epoch/(hyper_batch_size)),\n",
    "                                 'lr_decay':0.95,\n",
    "                                 'cycle_length': 5,\n",
    "                                 'mult_factor': 1.5}\n",
    "        \n",
    "        config = {'learning_rate':0.001, 'SGDRScheduler_config': SGDRScheduler_config}\n",
    "        \n",
    "        return [model, config]    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # ------------------------------------------ sequential definition --------------------------------------------------------------------\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def make_seq_cnn_blstm():\n",
    "        \n",
    "        # regularization: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8 ,input_shape=( inputSeqLength, totalNuclType ))  )\n",
    "        model.add(Conv1D(filters=32, kernel_size=12 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.4)) # input = 129 * 32\n",
    "        model.add(MaxPooling1D(pool_size= 2 ))#totalNuclType)) # input = 129 * 32 , output= 32*32 (filter/pool_size* filterTotal)\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "        model.add(Conv1D(filters=32, kernel_size=12, kernel_regularizer=l2(4e-4), bias_regularizer=l2(4e-4) )  )    \n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "        model.add(MaxPooling1D(pool_size=2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32\n",
    "\n",
    "        #model.add(Conv1D(filters=32, kernel_size=8))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "        model.add(Conv1D(filters=32, kernel_size=12, kernel_regularizer=l2(8e-4), bias_regularizer=l2(8e-4) )  )    \n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "        model.add(MaxPooling1D(pool_size =2 ))#totalNuclType))  # input = 25 *32 , output 6 * 32    \n",
    "\n",
    "    #     model.add(Conv1D(filters=92, kernel_size=3))#, input_shape=(32, 32))) # inputSeqLength, totalNuclType\n",
    "    #     #model.add(Conv1D(filters=32, kernel_size=12 ,input_shape=( inputSeqLength, totalNuclType ), kernel_regularizer=l2(4e-5), bias_regularizer=l2(4e-5) )  )    \n",
    "    #     #model.add(BatchNormalization())    \n",
    "    #     model.add(Activation('relu'))\n",
    "    #     model.add(Dropout(rate = 0.4)) # (32-8)/1  + 1 = 25 , output = 25 *32\n",
    "    #     model.add(MaxPooling1D(pool_size=totalNuclType))  # input = 25 *32 , output 6 * 32  \n",
    "\n",
    "\n",
    "        #model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "        model.add(Bidirectional(CuDNNGRU(240, return_sequences=True, kernel_regularizer=l2(4e-3), recurrent_regularizer=l2(5e-3), bias_regularizer=l2(4e-3)) ))\n",
    "        model.add(Dropout(rate = 0.6))    \n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        #model.add(Dense(16))\n",
    "        #  cite: use weight decay increasing from 0 to 1E-5 at 1200 epochs, to 1E-4 at 2500 epochs, and to 1E-3 at 400 epochs. \n",
    "        # [â€¦] The surface is smoother and transitions are more gradual\n",
    "        model.add(Dense(16, kernel_regularizer=l2(0.01) ) )\n",
    "        model.add(BatchNormalization())    \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(rate = 0.5)) \n",
    "        model.add(Dense(2, kernel_regularizer=l2(0.01)))        \n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "#         SGDRScheduler_config = { 'min_lr':2e-4,\n",
    "#                                  'max_lr':.2e-3,\n",
    "#                                  'steps_per_epoch':np.ceil(hyper_epoch/(hyper_batch_size)),\n",
    "#                                  'lr_decay':0.95,\n",
    "#                                  'cycle_length': 5,\n",
    "#                                  'mult_factor': 1.5}\n",
    "\n",
    "#         SGDRScheduler_config = { 'min_lr':5e-4,\n",
    "#                                  'max_lr':.5e-3,\n",
    "#                                  'steps_per_epoch':np.ceil(hyper_epoch/(hyper_batch_size)),\n",
    "#                                  'lr_decay':0.95,\n",
    "#                                  'cycle_length': 2,\n",
    "#                                  'mult_factor': 1.5}        \n",
    "\n",
    "\n",
    "        SGDRScheduler_config = { 'min_lr':1e-4,\n",
    "                                 'max_lr': .5e-3,\n",
    "                                 'steps_per_epoch':np.ceil(hyper_epoch/(hyper_batch_size)),\n",
    "                                 'lr_decay':0.95,\n",
    "                                 'cycle_length': 2,\n",
    "                                 'mult_factor': 1.5}        \n",
    "        \n",
    "        config = {'learning_rate':0.002, 'SGDRScheduler_config': SGDRScheduler_config}\n",
    "        \n",
    "        return [model, config]    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # ------------------------------------------ functional definition --------------------------------------------------------------------        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def make_func_inception():\n",
    "        input_2d = Input(shape = ( inputSeqLength, totalNuclType ))\n",
    "\n",
    "        def create_inception_module(input_tensor):\n",
    "            c3 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
    "            c5 = Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(input_tensor)\n",
    "            c8 = Conv1D(filters=32, kernel_size=8, padding='same', activation='relu')(input_tensor)\n",
    "            #c12 = Conv1D(filters=32, kernel_size=12, padding='same', activation='relu')(input_tensor)\n",
    "            #inception_module = Concatenate([c3, c5, c8, c12], axis = 2)\n",
    "            inception_module = Concatenate([c3, c5, c8], axis = 2)\n",
    "            \n",
    "            \n",
    "            #p3 = MaxPooling1D(pool_size=2, strides=1, padding='same')(input_tensor)\n",
    "            #pool = MaxPooling1D(pool_size = totalNuclType, strides=1, padding='same')(c8)\n",
    "            #inception_module = c8\n",
    "\n",
    "            return inception_module\n",
    "\n",
    "        out = create_inception_module(input_2d)\n",
    "        #out = Dropout(rate = 0.1)(out)  # was 0.4\n",
    "        out = Dropout(rate = 0.4)(out)  # was 0.4\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)\n",
    "\n",
    "\n",
    "        out = create_inception_module(out)\n",
    "        #out = Dropout(rate = 0.1)(out) # was 0.4\n",
    "        out = Dropout(rate = 0.4)(out) # was 0.4\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)\n",
    "\n",
    "        out = Conv1D(filters=32, kernel_size=8)(out)\n",
    "        out = Activation('relu')(out)     \n",
    "        #out = Dropout(rate = 0.45)(out) # was 0.4\n",
    "        out = Dropout(rate = 0.4)(out) # was 0.4\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)\n",
    "\n",
    "    #     out = Bidirectional( LSTM(16, return_sequences=True))(out)\n",
    "    #     out = Bidirectional( LSTM(16, return_sequences=True))(out)    \n",
    "    #     out = Dropout(rate = 0.25)(out)\n",
    "\n",
    "        out = Flatten()(out)\n",
    "        out = Dense(16)(out)\n",
    "        out = Activation('relu')(out)\n",
    "        out = Dropout(rate = 0.5)(out)\n",
    "\n",
    "        #out = Dense(2, activation='softmax')(out)\n",
    "        out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "        model = Model(inputs = input_2d, outputs = out)        \n",
    "    \n",
    "        SGDRScheduler_config = { 'min_lr':3e-5,\n",
    "                                 'max_lr':1e-3,    #0.5e-3, #1e-3,\n",
    "                                 'steps_per_epoch':np.ceil(hyper_epoch/(5*hyper_batch_size)),\n",
    "                                 'lr_decay':0.95,\n",
    "                                 'cycle_length':10,\n",
    "                                 'mult_factor': 1.0}\n",
    "        \n",
    "        config = {'learning_rate':0.002, 'SGDRScheduler_config': SGDRScheduler_config}\n",
    "        \n",
    "        return [model, config]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # ------------------------------------------ functional definition --------------------------------------------------------------------        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def make_func_inception_small():\n",
    "        input_2d = Input(shape = ( inputSeqLength, totalNuclType ))\n",
    "\n",
    "        def create_inception_module(input_tensor):\n",
    "            c3 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
    "            c5 = Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(input_tensor)\n",
    "            c8 = Conv1D(filters=32, kernel_size=8, padding='same', activation='relu')(input_tensor)\n",
    "            #c12 = Conv1D(filters=32, kernel_size=12, padding='same', activation='relu')(input_tensor)\n",
    "            #inception_module = Concatenate([c3, c5, c8, c12], axis = 2)\n",
    "            inception_module = Concatenate([c3, c5, c8], axis = 2)\n",
    "            \n",
    "            \n",
    "            #p3 = MaxPooling1D(pool_size=2, strides=1, padding='same')(input_tensor)\n",
    "            #pool = MaxPooling1D(pool_size = totalNuclType, strides=1, padding='same')(c8)\n",
    "            #inception_module = c8\n",
    "\n",
    "            return inception_module\n",
    "\n",
    "        def create_resnet_module(input_tensor):\n",
    "            c3 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(input_tensor)\n",
    "            c5 = Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(input_tensor)\n",
    "            c8 = Conv1D(filters=32, kernel_size=8, padding='same', activation='relu')(input_tensor)\n",
    "            #c12 = Conv1D(filters=32, kernel_size=12, padding='same', activation='relu')(input_tensor)\n",
    "            #inception_module = Concatenate([c3, c5, c8, c12], axis = 2)\n",
    "            inception_module = Concatenate([c3, c5, c8, input_tensor], axis = 2)\n",
    "            \n",
    "            \n",
    "            #p3 = MaxPooling1D(pool_size=2, strides=1, padding='same')(input_tensor)\n",
    "            #pool = MaxPooling1D(pool_size = totalNuclType, strides=1, padding='same')(c8)\n",
    "            #inception_module = c8\n",
    "\n",
    "            return inception_module        \n",
    "        \n",
    "        out = create_inception_module(input_2d)\n",
    "        #out = Dropout(rate = 0.1)(out)  # was 0.4\n",
    "        out = Dropout(rate = 0.4)(out)  # was 0.4\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)\n",
    "        \n",
    "        out = create_inception_module(out)\n",
    "        #out = Dropout(rate = 0.1)(out) # was 0.4\n",
    "        out = Dropout(rate = 0.5)(out) # was 0.4\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)   \n",
    "\n",
    "#         out = create_resnet_module(out)\n",
    "#         #out = Dropout(rate = 0.1)(out) # was 0.4\n",
    "#         out = Dropout(rate = 0.4)(out) # was 0.4\n",
    "#         out = MaxPooling1D(pool_size = totalNuclType)(out)   \n",
    "        \n",
    "        out = Conv1D(filters=64, kernel_size=8, kernel_regularizer=l2(8e-4), bias_regularizer=l2(8e-4))(out)\n",
    "        #out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)     \n",
    "        #out = Dropout(rate = 0.45)(out) # was 0.4\n",
    "        out = Dropout(rate = 0.6)(out) # was 0.5, and 0.4 before\n",
    "        out = MaxPooling1D(pool_size = totalNuclType)(out)\n",
    "\n",
    "        \n",
    "#         out = Bidirectional( LSTM(16, return_sequences=True))(out)\n",
    "#         out = Bidirectional( LSTM(16, return_sequences=True))(out)    \n",
    "#         out = Dropout(rate = 0.25)(out)\n",
    "\n",
    "        out = Flatten()(out)\n",
    "        out = Dense(16, kernel_regularizer=l2(0.02))(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Activation('relu')(out)\n",
    "        out = Dropout(rate = 0.7)(out) # was 0.5\n",
    "\n",
    "        #out = Dense(2, activation='softmax')(out)\n",
    "        out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "        model = Model(inputs = input_2d, outputs = out)        \n",
    "    \n",
    "        SGDRScheduler_config = { 'min_lr':3e-5,\n",
    "                                 'max_lr':1e-3,    #0.5e-3, #1e-3,\n",
    "                                 'steps_per_epoch':np.ceil(hyper_epoch/(5*hyper_batch_size)),\n",
    "                                 'lr_decay':0.85,\n",
    "                                 'cycle_length':10,\n",
    "                                 'mult_factor': 1.0}\n",
    "        \n",
    "        config = {'learning_rate':0.002, 'SGDRScheduler_config': SGDRScheduler_config}\n",
    "        \n",
    "        return [model, config]    \n",
    "    \n",
    "    \n",
    "    return make_func_inception_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# y_true = [2, 0, 2, 2, 0, 1]\n",
    "# y_pred = [0, 0, 2, 2, 0, 2]\n",
    "# confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "# y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "# confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "\n",
    "# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0])\n",
    "# (tn, fp, fn, tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=23, shuffle=True)\n",
      "\n",
      "\n",
      " --------------- Fold: 1  ---------------\n",
      "train fold size: 33546 \n",
      "Validation fold size: 3728\n",
      "number of samples in Training: 33546  seq length: 1000\n",
      "number of samples in Testing : 3728  seq length: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 09:38:02.756501 140094755354432 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0728 09:38:02.826603 140094755354432 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in used dataset: 37274  seq length: 1000\n",
      "Building model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 09:38:02.925521 140094755354432 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1000, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 1000, 64)     832         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1000, 64)     1344        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1000, 32)     1056        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000, 160)    0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000, 160)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 250, 160)     0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 250, 64)      30784       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 250, 64)      51264       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 250, 32)      40992       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 250, 160)     0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 250, 160)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 62, 160)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 55, 64)       81984       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 55, 64)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 55, 64)       0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 13, 64)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 832)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           13328       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            17          dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 221,665\n",
      "Trainable params: 221,633\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Running at most 500 epochs\n",
      "\n",
      "************************************************************ \n",
      "************** TRAINING START (fold 1 ) **************** \n",
      "Train on 33546 samples, validate on 3728 samples\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_7/conv1d}}]]\n\t [[metrics_2/sensitivity/Identity/_431]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_7/conv1d}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5107ba7f4277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     history = model.fit(train_features, train_labels, epochs= hyper_epoch, batch_size=hyper_batch_size, shuffle=True, verbose=1, \n\u001b[0;32m--> 183\u001b[0;31m                         validation_data = (test_features, test_labels), callbacks=[schedule, best_checkpoint, every_epoch_checkpoint, loss_plotter, metrics] )    \n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;31m#class_weight=class_weight ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_7/conv1d}}]]\n\t [[metrics_2/sensitivity/Identity/_431]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_7/conv1d}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################### Stratified k-fold Cross validation ###############\n",
    "\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(input_features, input_labels,\n",
    "#                                                                                 test_size=hyper_test_split,\n",
    "#                                                                                 stratify=input_labels,\n",
    "#                                                                                 random_state=42)\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    \n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    \n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.val_sensitivity = []\n",
    "        self.val_specificity = []\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print(dir(self.model))\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        cm = confusion_matrix(val_targ, val_predict)\n",
    "#         output = \"\"\n",
    "#         output = output + 'Confusion matrix:\\n' + str(cm) \n",
    "#         output += \"\\n\"\n",
    "\n",
    "#         print('Confusion matrix:\\n' ,cm)\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "#         total1=sum(sum(cm))\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        #tp=cm[0,0];fn=cm[1,0];fp=cm[0,1];tn=cm[1,1];\n",
    "\n",
    "#         accuracy1=(tp+tn)/total1\n",
    "\n",
    "        _val_specificity = tn/(fp+tn)\n",
    "        _val_sensitivity = tp/(tp+fn)\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(f'\\n â€” val_f1: {_val_f1} â€” val_precision: {_val_precision} â€” val_recall {_val_recall}')\n",
    "        print(f' â€” val_sensitivity: {_val_sensitivity} â€” _val_specificity: {_val_specificity}\\n')\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "dfROC =  pd.DataFrame(data=None, columns=['AREA_ROC'] )   \n",
    "dfROC_best =  pd.DataFrame(data=None, columns=['AREA_ROC'] )   \n",
    "dfROC_train =  pd.DataFrame(data=None, columns=['AREA_ROC'] )   \n",
    "dfROC_best_train =  pd.DataFrame(data=None, columns=['AREA_ROC'] )   \n",
    "\n",
    "dfPR  =  pd.DataFrame(data=None, columns=['AREA_PR' ] )\n",
    "dfPR_best  =  pd.DataFrame(data=None, columns=['AREA_PR' ] )\n",
    "dfPR_train  =  pd.DataFrame(data=None, columns=['AREA_PR' ] )\n",
    "dfPR_best_train  =  pd.DataFrame(data=None, columns=['AREA_PR' ] )\n",
    "\n",
    "\n",
    "dfCM      = pd.DataFrame(data=None, columns=['tp', 'fn' , 'fp' , 'tn'])\n",
    "dfCM_best = pd.DataFrame(data=None, columns=['tp', 'fn' , 'fp' , 'tn'])\n",
    "dfCM_train      = pd.DataFrame(data=None, columns=['tp', 'fn' , 'fp' , 'tn'])\n",
    "dfCM_best_train = pd.DataFrame(data=None, columns=['tp', 'fn' , 'fp' , 'tn'])    \n",
    "\n",
    "training_histories = []\n",
    "last_model_performance_histories = []\n",
    "best_model_performance_histories = []\n",
    "\n",
    "last_model_performance_histories_trainset = []\n",
    "best_model_performance_histories_trainset = []\n",
    "\n",
    "\n",
    "skf = StratifiedKFold( n_splits= k_folds , random_state = 23, shuffle=True)\n",
    "print(skf)  \n",
    "k=0\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(sequences, labels):\n",
    "    \n",
    "    # fold K\n",
    "    k=k+1;\n",
    "    \n",
    "    print()    \n",
    "    print()    \n",
    "    print(\" --------------- Fold:\", k, \" ---------------\")\n",
    "    print(\"train fold size:\", len(train_index ), \"\\nValidation fold size:\", len(test_index) )\n",
    "\n",
    "    # get the features and labels for the train set\n",
    "    train_features = input_features[train_index]\n",
    "    train_labels   = labels[train_index]#input_labels[train_index]\n",
    "    \n",
    "    # get the features and labels for the validation set\n",
    "    test_features = input_features[test_index]\n",
    "    test_labels   = labels[test_index]#input_labels[test_index] \n",
    "\n",
    "    # feature dimension\n",
    "    inputSeqLength = len(train_features[0]);\n",
    "    inputSeqLength = np.array(train_features).shape[1];\n",
    "    \n",
    "    size0 = np.array(train_features).shape[0];\n",
    "    size1 = np.array(train_features).shape[1];\n",
    "    print(\"number of samples in Training: \" + str(size0) + \"  seq length: \" + str(size1))\n",
    "\n",
    "    size0 = np.array(test_features).shape[0];\n",
    "    size1 = np.array(test_features).shape[1];\n",
    "    print(\"number of samples in Testing : \" + str(size0) + \"  seq length: \" + str(size1))\n",
    "\n",
    "    size0 = np.array(input_features).shape[0];\n",
    "    size1 = np.array(input_features).shape[1];\n",
    "    print(\"number of samples in used dataset: \" + str(size0) + \"  seq length: \" + str(size1))\n",
    "        \n",
    "    print('Building model')\n",
    "\n",
    "    model, config = make_model()\n",
    "\n",
    "    learning_rate = config['learning_rate']\n",
    "\n",
    "#     decay_rate = learning_rate / hyper_epoch\n",
    "\n",
    "    adamOpt = Adam(lr = 0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad = False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adamOpt, metrics=[sensitivity, specificity,'binary_accuracy']) # 'adam' , metrics=['binary_accuracy', my_pred]\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=adamOpt, metrics=[sensitivity, specificity,'binary_accuracy']) # 'adam' , metrics=['binary_accuracy', my_pred]\n",
    "    \n",
    "    # Adadelta didn't give us good result. we omitted it.\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['binary_accuracy']) # 'adam' , metrics=['binary_accuracy', my_pred]\n",
    "    \n",
    "    print ( model.summary() )\n",
    "    \n",
    "    \n",
    "    print('Running at most', hyper_epoch, 'epochs')\n",
    "\n",
    "    print()\n",
    "    print(\"************************************************************ \")        \n",
    "    print(\"************** TRAINING START (fold\", k, \") **************** \")\n",
    "    # https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "    \n",
    "    fileWeight_BestModel = f\"{snapshot_dir}{k}_Weights_bestModel.h5\"\n",
    "    every_epoch_checkpoint_fname = snapshot_dir+\"saved-model-{epoch:02d}-{val_loss:.2f}-fold_\"+str(k)+\".hdf5\"\n",
    "    \n",
    "    #     monitor='val_loss', mode='min'\n",
    "    #     monitor='val_acc', mode='max'\n",
    "    #     monitor='val_fmeasure', mode='max'\n",
    "    \n",
    "    \n",
    "    best_checkpoint = ModelCheckpoint(fileWeight_BestModel, monitor='val_loss',mode='auto', save_best_only=True, verbose=1, save_weights_only=True )\n",
    "    every_epoch_checkpoint = ModelCheckpoint(filepath = every_epoch_checkpoint_fname, monitor='val_loss', verbose=1, save_best_only=False, \n",
    "                                             save_weights_only=False, mode='auto', period=1)\n",
    "    #     val_loss,val_binary_accuracy,loss,binary_accuracy\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    schedule = SGDRScheduler(min_lr = config['SGDRScheduler_config']['min_lr'],\n",
    "                             max_lr = config['SGDRScheduler_config']['max_lr'],\n",
    "                             steps_per_epoch = config['SGDRScheduler_config']['steps_per_epoch'],\n",
    "                             lr_decay = config['SGDRScheduler_config']['lr_decay'],\n",
    "                             cycle_length = config['SGDRScheduler_config']['cycle_length'],\n",
    "                             mult_factor = config['SGDRScheduler_config']['mult_factor'])\n",
    "    \n",
    "    # for plotting losses        \n",
    "    loss_plotter = PlotLosses()\n",
    "\n",
    "    metrics = Metrics(val_data  = (test_features, test_labels))\n",
    "\n",
    "    history = model.fit(train_features, train_labels, epochs= hyper_epoch, batch_size=hyper_batch_size, shuffle=True, verbose=1, \n",
    "                        validation_data = (test_features, test_labels), callbacks=[schedule, best_checkpoint, every_epoch_checkpoint, loss_plotter, metrics] )    \n",
    "    #class_weight=class_weight , \n",
    "    \n",
    "    training_histories.append(history)\n",
    "\n",
    "    # using validation set fold for validation\n",
    "    #history = model.fit(train_features, train_labels, epochs= hyper_epoch, batch_size=hyper_batch_size, shuffle=True, verbose=1, \n",
    "    #                    validation_data = (test_features, test_labels), class_weight=class_weight , callbacks=[early_stopping, checkpoint] )  \n",
    "    \n",
    "    \n",
    "    # save the last iteration's model and weights\n",
    "    fileModel = f'{snapshot_dir}{k}_Model.h5'\n",
    "    model.save(fileModel)\n",
    "    fileWeight = f'{snapshot_dir}{k}_Weights.h5'\n",
    "    model.save_weights(fileWeight)\n",
    "\n",
    "    print(\"************** TRAINING END (fold\", k, \") **************** \")\n",
    "    print(\"********************************************************** \")        \n",
    "\n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    \n",
    "    last_model_weight_file = f'{snapshot_dir}{k}_Weights.h5'\n",
    "    print(last_model_weight_file)\n",
    "    model.load_weights(last_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, test_features, test_labels, dfCM, dfROC, dfPR )\n",
    "    last_model_performance_histories.append(test_summary)    \n",
    "        \n",
    "    print(\"************** TESTING END (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")            \n",
    "    \n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    best_model_weight_file = f\"{snapshot_dir}{k}_Weights_bestModel.h5\"\n",
    "    print(fileWeight_BestModel)\n",
    "    model.load_weights(best_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, test_features, test_labels, dfCM_best, dfROC_best, dfPR_best )\n",
    "    best_model_performance_histories.append(test_summary)    \n",
    "    \n",
    "    print(\"************** TESTING END  (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")   \n",
    "\n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START ON TRAINING SET (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    last_model_weight_file = f'{snapshot_dir}{k}_Weights.h5'\n",
    "    print(last_model_weight_file)\n",
    "    model.load_weights(last_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, train_features, train_labels, dfCM_train, dfROC_train, dfPR_train )\n",
    "    last_model_performance_histories_trainset.append(test_summary) \n",
    "    \n",
    "    print(\"************** TESTING END ON TRAINING SET (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")\n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START ON TRAINING SET (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    best_model_weight_file = f\"{snapshot_dir}{k}_Weights_bestModel.h5\"\n",
    "    print(fileWeight_BestModel)\n",
    "    model.load_weights(best_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, train_features, train_labels, dfCM_best_train, dfROC_best_train, dfPR_best_train )\n",
    "    best_model_performance_histories_trainset.append(test_summary)\n",
    "    \n",
    "    print(\"************** TESTING END ON TRAINING SET (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")    \n",
    "    \n",
    "##########################  Split End ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test( mdl, features, labels, cm_df, roc_df, dpr_df ):\n",
    "    \n",
    "    predicted_labels = (np.asarray(mdl.predict(np.stack(features)))).round()\n",
    "    \n",
    "    \n",
    "    #cm = confusion_matrix(np.argmax(labels, axis=1), np.argmax(predicted_labels, axis=1))\n",
    "    cm = confusion_matrix(labels, predicted_labels)\n",
    "    output = \"\"\n",
    "    output = output + 'Confusion matrix:\\n' + str(cm) \n",
    "    output += \"\\n\"\n",
    "    \n",
    "    print('Confusion matrix:\\n' ,cm)\n",
    "    # cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    total1=sum(sum(cm))\n",
    "    tp=cm[0,0];fn=cm[0,1];fp=cm[1,0];tn=cm[1,1];\n",
    "    \n",
    "    accuracy1=(tp+tn)/total1\n",
    "    sensitivity1 = tp/(tp+fn)\n",
    "    specificity1 = tn/(fp+tn)\n",
    "    if (tp)==0:\n",
    "        precision1 = 0\n",
    "    else:\n",
    "        precision1 = tp / (tp + fp)\n",
    "\n",
    "    MCC = (tp*tn - fp*fn) / np.sqrt(  (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  )    \n",
    "    print( \"Precision : \" + str(precision1) + \" Sensitivity/Recall: \" + str (sensitivity1) + \" Specificity: \" + str(specificity1 )  + \" Accuracy: \" + str(accuracy1 ) + \" MCC: \" + str(MCC )) \n",
    "\n",
    "    output = output + \"Precision : \" + str(precision1) + \" Sensitivity/Recall: \" + str (sensitivity1) + \" Specificity: \" + str(specificity1 )  + \" Accuracy: \" + str(accuracy1 ) + \" MCC: \" + str(MCC )\n",
    "    output += \"\\n\"\n",
    "    \n",
    "    cm_df =  dfCM.append( {  'tp': tp , 'fn' : fn , 'fp': fp  , 'tn' : tn  } , ignore_index=True)   \n",
    "    \n",
    "    #auc_roc           = roc_auc_score( np.argmax(labels, axis=1), np.argmax(predicted_labels, axis=1) )  \n",
    "    auc_roc           = roc_auc_score( labels, predicted_labels )  \n",
    "    #average_precision = average_precision_score(np.argmax(labels, axis=1), np.argmax(predicted_labels, axis=1))\n",
    "    average_precision = average_precision_score(labels, predicted_labels )\n",
    "    print( \"area ROC : \" , auc_roc ,  \" area PR \" , average_precision)\n",
    "    output = output + \" area ROC : \" + str(auc_roc) +  \" area PR \" + str(average_precision)\n",
    "    output += \"\\n\"\n",
    "\n",
    "    roc_df =  roc_df.append( {  'AREA_ROC': auc_roc  } , ignore_index=True) \n",
    "    dpr_df  =  dpr_df.append( {  'AREA_PR': average_precision  } , ignore_index=True) \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test_metrics():\n",
    "    k = 1\n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    \n",
    "    last_model_weight_file = f'{snapshot_dir}saved-model-114-0.40-fold_{k}.hdf5'\n",
    "    print(last_model_weight_file)\n",
    "    model.load_weights(last_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, test_features, test_labels, dfCM, dfROC, dfPR )\n",
    "    last_model_performance_histories.append(test_summary)    \n",
    "        \n",
    "    print(\"************** TESTING END (LAST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")            \n",
    "   \n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    best_model_weight_file = f\"{snapshot_dir}{k}_Weights_bestModel.h5\"\n",
    "    print(fileWeight_BestModel)\n",
    "    model.load_weights(best_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, test_features, test_labels, dfCM_best, dfROC_best, dfPR_best )\n",
    "    best_model_performance_histories.append(test_summary)    \n",
    "    \n",
    "    print(\"************** TESTING END  (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \")   \n",
    "\n",
    "    \n",
    "#     print(\"************************************************************************ \")            \n",
    "#     print(\"************** TESTING START ON TRAINING SET (LAST MODEL) (fold\", k, \") **************** \")\n",
    "#     last_model_weight_file = f'{snapshot_dir}{k}_Weights.h5'\n",
    "#     print(last_model_weight_file)\n",
    "#     model.load_weights(last_model_weight_file, by_name=False)\n",
    "\n",
    "#     test_summary = perform_test(model, train_features, train_labels, dfCM_train, dfROC_train, dfPR_train )\n",
    "#     last_model_performance_histories_trainset.append(test_summary) \n",
    "    \n",
    "#     print(\"************** TESTING END ON TRAINING SET (LAST MODEL) (fold\", k, \") **************** \")\n",
    "#     print(\"************************************************************************ \")\n",
    "    \n",
    "    print(\"************************************************************************ \")            \n",
    "    print(\"************** TESTING START ON TRAINING SET (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    best_model_weight_file = f\"{snapshot_dir}{k}_Weights_bestModel.h5\"\n",
    "    print(fileWeight_BestModel)\n",
    "    model.load_weights(best_model_weight_file, by_name=False)\n",
    "\n",
    "    test_summary = perform_test(model, train_features, train_labels, dfCM_best_train, dfROC_best_train, dfPR_best_train )\n",
    "    best_model_performance_histories_trainset.append(test_summary)\n",
    "    \n",
    "    print(\"************** TESTING END ON TRAINING SET (BEST MODEL) (fold\", k, \") **************** \")\n",
    "    print(\"************************************************************************ \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "test_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_training_histories():\n",
    "    \n",
    "    for history in training_histories:\n",
    "        # Plot training & validation accuracy values\n",
    "#         plt.plot(history.history['binary_accuracy'])\n",
    "#         plt.plot(history.history['val_binary_accuracy'])\n",
    "#         plt.title('Model accuracy')\n",
    "#         plt.ylabel('Accuracy')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#         plt.show()\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()    \n",
    "        \n",
    "#plot_training_histories()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load a model from file and evaluate its performance\n",
    "#\n",
    "#\n",
    "\n",
    "# dfROC_best_val =  pd.DataFrame(data=None, columns=['AREA_ROC'] )   \n",
    "\n",
    "# dfPR_best_val  =  pd.DataFrame(data=None, columns=['AREA_PR' ] )\n",
    "\n",
    "# dfCM_best_val = pd.DataFrame(data=None, columns=['tp', 'fn' , 'fp' , 'tn'])    \n",
    "\n",
    "# for k in range(1,11):\n",
    "#     best_model_weight_file = f\"{ds_dir}{k}_Weights_bestModel.h5\"\n",
    "#     print(\"************************* \",k, \" **************************\")\n",
    "#     print(fileWeight_BestModel)\n",
    "#     model.load_weights(best_model_weight_file, by_name=False)\n",
    "\n",
    "#     perform_test(model, test_features, test_labels, dfCM_best_val, dfROC_best_val, dfPR_best_val )\n",
    "#     print()\n",
    "#     print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate loss plots for all the folds along with the corresponding performance metrics\n",
    "# and write the result to a PDF file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "\n",
    "with PdfPages('test.pdf') as pdf:\n",
    "    t = np.arange(0.0, 2.0, 0.01)\n",
    "    s = 1 + np.sin(2*np.pi*t)\n",
    "    s = s * 50\n",
    "\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    n=0\n",
    "    \n",
    "    for history in training_histories:\n",
    "        n += 1\n",
    "        ax = fig.add_subplot(4,3,n)\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        ax.plot(history.history['loss'])\n",
    "        ax.plot(history.history['val_loss'])\n",
    "        ax.yaxis.set_label_text('Loss')\n",
    "        ax.xaxis.set_label_text('Epoch')\n",
    "        ax.legend(['Train', 'Test'], loc='upper right')            \n",
    "\n",
    "    pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*********** AREA ROC for BEST model *************')\n",
    "\n",
    "for index, row in dfROC_best.iterrows():\n",
    "        print(  row['AREA_ROC']  )\n",
    "        \n",
    "print( dfROC_best.mean(axis = 0, skipna = True)  )\n",
    "\n",
    "print('*********** AREA ROC for BEST model END *************')\n",
    "\n",
    "\n",
    "print('*********** AREA PR for BEST model *************')\n",
    "\n",
    "for index, row in dfPR_best.iterrows():\n",
    "        print(  row['AREA_PR']  )\n",
    "        \n",
    "print( dfPR_best.mean(axis = 0, skipna = True)  )\n",
    "\n",
    "print('*********** AREA PR for BEST model END *************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*********** CV  for last iteration model *************')\n",
    "\n",
    "for index, row in dfCM.iterrows():\n",
    "        print(  row['tp'], row['fn'] ,  row['fp'], row['tn']  )\n",
    "\n",
    "print( dfCM.sum(axis = 0, skipna = True)  )\n",
    "\n",
    "dfTotal=dfCM.sum(axis = 0, skipna = True) \n",
    "total_tp = dfTotal['tp'];\n",
    "total_fn = dfTotal['fn'];\n",
    "total_fp = dfTotal['fp'];\n",
    "total_tn = dfTotal['tn'];\n",
    "total_all = total_tp + total_fn + total_fp + total_tn;\n",
    "accuracy1=(total_tp+total_tn)/total_all\n",
    "sensitivity1 = total_tp/(total_tp+total_fn)\n",
    "specificity1 = total_tn/(total_fp+total_tn)\n",
    "if (total_tp)==0:\n",
    "    precision1 = 0\n",
    "else:\n",
    "    precision1 = total_tp / (total_tp + total_fp)\n",
    "\n",
    "MCC = (total_tp*total_tn - total_fp*total_fn) / np.sqrt(  (total_tp+total_fp)*(total_tp+total_fn)*(total_tn+total_fp)*(total_tn+total_fn)  ) \n",
    "\n",
    "print(\"TOTAL RECORDS IN ALL FOLD OF CV \" , total_all)\n",
    "print(  \"Precision : \" + str(precision1) + \" Sensitivity/Recall: \" + str (sensitivity1) + \" Specificity: \" + str(specificity1 )  + \" Accuracy: \" + str(accuracy1 ) + \" MCC: \" + str(MCC ))\n",
    "\n",
    "\n",
    "\n",
    "print('*********** CV  for best model *************')\n",
    "for index, row in dfCM_best.iterrows():\n",
    "        print(  row['tp'], row['fn'] ,  row['fp'], row['tn']  )\n",
    "        \n",
    "        \n",
    "print( dfCM_best.sum(axis = 0, skipna = True)  )\n",
    "\n",
    "dfTotal=dfCM_best.sum(axis = 0, skipna = True) \n",
    "total_tp = dfTotal['tp'];\n",
    "total_fn = dfTotal['fn'];\n",
    "total_fp = dfTotal['fp'];\n",
    "total_tn = dfTotal['tn'];\n",
    "total_all = total_tp + total_fn + total_fp + total_tn;\n",
    "accuracy1=(total_tp+total_tn)/total_all\n",
    "sensitivity1 = total_tp/(total_tp+total_fn)\n",
    "specificity1 = total_tn/(total_fp+total_tn)\n",
    "if (total_tp)==0:\n",
    "    precision1 = 0\n",
    "else:\n",
    "    precision1 = total_tp / (total_tp + total_fp)\n",
    "\n",
    "MCC = (total_tp*total_tn - total_fp*total_fn) / np.sqrt(  (total_tp+total_fp)*(total_tp+total_fn)*(total_tn+total_fp)*(total_tn+total_fn)  ) \n",
    "\n",
    "print(\"TOTAL RECORDS IN ALL FOLD OF CV \" , total_all)\n",
    "print(  \"Precision : \" + str(precision1) + \" Sensitivity/Recall: \" + str (sensitivity1) + \" Specificity: \" + str(specificity1 )  + \" Accuracy: \" + str(accuracy1 ) + \" MCC: \" + str(MCC ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################  PR curve Drawing ###############################\n",
    "\n",
    "test_label_rep = test_labels\n",
    "pred_data = predicted_labels\n",
    "cm1 = confusion_matrix(np.argmax(test_labels, axis=1), np.argmax(predicted_labels, axis=1))\n",
    "\n",
    "\n",
    "print(\"********* Precision-Recall Curve *********\")\n",
    "\n",
    "average_precision = average_precision_score(np.argmax(test_labels, axis=1), np.argmax(predicted_labels, axis=1))\n",
    "print('Average Precision (AUPR)' , average_precision)\n",
    "precision, recall, _ = precision_recall_curve(np.argmax(test_labels, axis=1), np.argmax(predicted_labels, axis=1))\n",
    "\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "##########################  PR curve Drawing END ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################  ROC curve Drawing START ###############################\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "\n",
    "\n",
    "print(\"********* Receiver Operating Curve *********\")\n",
    "print ('AUC: '+str(roc_auc_score( np.argmax(test_labels, axis=1), np.argmax(predicted_labels, axis=1) ) ) )\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(  np.argmax(test_labels, axis=1), np.argmax(predicted_labels, axis=1)  )\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "lw=2;\n",
    "plt.plot(fpr, tpr, lw=lw, alpha=0.3, label='ROC'  )\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##########################  ROCcurve Drawing END ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Cross-validated ROC curve Drawing START ###############################\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "\n",
    "\n",
    "print(cm)\n",
    "print(history)\n",
    "\n",
    "import json\n",
    "# Get the dictionary containing each metric and the loss for each epoch\n",
    "#history_dict = history\n",
    "#print(history_dict['loss'][49])\n",
    "\n",
    "########################## Cross-validated ROC curve Drawing END ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
